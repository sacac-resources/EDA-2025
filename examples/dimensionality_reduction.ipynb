{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = {\n",
    "    'iof_data_1min_csv' : \"https://drive.google.com/uc?id=1_jYVXj7mt8Zzpjn8WGI111G-kWRTbfjU\",\n",
    "    'iof_data_1min_parq' : \"https://drive.google.com/uc?id=1j5SS136UzbSPu8TqG9RRUMi6-wWF9dzq\",\n",
    "    'mixingTank' :  \"https://drive.google.com/uc?id=1b5Qn5LIa6KAE03Tq4yRVdhTyUmZLxRjt\",\n",
    "    'moons' : \"https://drive.google.com/uc?id=1a9zTkPEpuHGj6LzGzuLe-JSLg_4GJef4\",\n",
    "    'open_iof_20min' : \"https://drive.google.com/uc?id=15lkhdBfWnjlpgpEx4T2XcRApKr-dmBb0\",\n",
    "    'open_iof_cleaned' : \"https://drive.google.com/uc?id=1WVbJvYsGy-iKlsW4WaDZrKy_NhK2tJLW\",\n",
    "    'test_data' : \"https://drive.google.com/uc?id=1AUJ63mIM2we9k6H6H149YH1mSg83d3H7\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pandas display format: two decimals\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to dimensionality reduction using the \"moons\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with a synthetic dataset to introduce the concept of PCA. Sometimes called the \"moons\" data set, it will also be useful in clustering later on. We load the data from `/data/moons_data.csv` then visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and described data\n",
    "df_moons = pd.read_csv(data_url['moons'])\n",
    "df_moons.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice there are three features, `x1`, `x2`, and `x3`, which we can plot in 3D. The second plot includes a command which allows you to change the angle at which the data is viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure, then plot the data in 3D\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(df_moons['x1'], df_moons['x2'], df_moons['x3'], alpha = 0.5)\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(df_moons['x1'], df_moons['x2'], df_moons['x3'], alpha = 0.5)\n",
    "ax2.view_init(elev=5, azim=10)  # Rotate the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotating the data in two dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rotate the plot using linear algebra. If $ Q $ is a matrix with orthogonal columns (i.e., $ q_i \\cdot q_j = 1 $ if $ i=j $, else $ q_i \\cdot q_j = 0 $), then multipyling our data matrix $ X $ by $ Q $ amounts to rotating the data. In particular,\n",
    "\n",
    "$$ Q = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\ \\end{bmatrix}  $$\n",
    "\n",
    "will rotate two-dimensional data. We will define $ T = XQ $, so that $ T $ represents the rotated data.\n",
    "\n",
    "Let's just consider the first two features in the moons data, then rotate it in space. You can change the amount of rotation by changing `theta` below (note that `theta` is in radians, not degrees). Also note that the `@` symbol can be used for matrix multiplication in Python (as opposed to `*`, which will result in element-wise multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data matrix using first two features\n",
    "X = df_moons[['x1', 'x2']].values\n",
    "\n",
    "# Create rotation matrix\n",
    "theta = 0/2*np.pi\n",
    "Q = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "# Rotate the data and plot\n",
    "T = X @ Q\n",
    "plt.scatter(T[:,0], T[:,1], alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to rotate the data so that we capture _as much variance as possible_ on the $ t_1 $-axis (that is, the first column in our rotated data matrix $ T $). To help with this, we also plot a histogram of $t_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_moons[['x1', 'x2']].values\n",
    "\n",
    "# Create rotation matrix\n",
    "theta = 0/4*np.pi\n",
    "Q = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "# Rotate the data and plot\n",
    "T = X @ Q\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,5))\n",
    "ax[0].hlines(0, -20, 20, color = 'k', alpha = 0.3)  # PLot the t1-axis\n",
    "ax[0].scatter(T[:,0], T[:,1], alpha = 0.5)\n",
    "ax[0].scatter(T[:,0], np.zeros_like(T[:,0]), c='k', alpha = 0.5)    # Show the values of T projected onto the t1-axis\n",
    "ax[0].set_xlim(-30, 30)\n",
    "ax[0].set_ylim(-30, 30)\n",
    "\n",
    "# Draw a histogram of t1\n",
    "ax[1].hist(T[:,0], bins=np.linspace(-30,30,50))\n",
    "ax[1].set_xlabel('t')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_title('Histogram of t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the standard deviation of $x_2$ is much larger than that of $x_1$ (run `df_moons.describe()` to confirm). Maximizing the variance in $t_1$ just amounts to rotating it so that $x_2$ is projected onto $t_1$, but this doesn't help us identify any _covariance_ or structure in our data. To address this problem, let us center and scale the data instead. \n",
    "\n",
    "It is recommended to center and scale data when performing dimensionality reduction. A common choice is to scale the data so that each feature has unit variance, which we can easily do this using `sklearn.preprocessing.StandardScaler`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StandardScaler from  scikit-learn, then define the scaled data \"X\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df_moons[['x1', 'x2']].values\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Rotate the scaled data and plot\n",
    "theta = 0/4*np.pi\n",
    "Q = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "T = X @ Q\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,5))\n",
    "ax[0].hlines(0, -20, 20, color = 'k', alpha = 0.3)\n",
    "ax[0].scatter(T[:,0], T[:,1], alpha = 0.5)\n",
    "ax[0].scatter(T[:,0], np.zeros_like(T[:,0]), c='k', alpha = 0.5)\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_xlim(-2.5, 2.5)\n",
    "ax[0].set_ylim(-2.5, 2.5)\n",
    "\n",
    "ax[1].hist(T[:,0], bins=np.linspace(-2.5,2.5,40))\n",
    "ax[1].set_xlabel('t')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_title('Histogram of t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotating data in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the three-dimensional data set. We can rotate data in 3D using the $ 3 × 3 $ orthogonal matrix $ Q $ defined by:\n",
    "\n",
    "$$ \n",
    "Q = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) & 0 \\\\ \\sin(\\theta) & \\cos(\\theta) & 0 \\\\ 0 & 0 & 1 \\ \\end{bmatrix}\n",
    "       \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(\\phi) & -\\sin(\\phi) \\\\ 0 & \\sin(\\phi) & \\cos(\\phi) \\ \\end{bmatrix}  \n",
    "$$\n",
    "\n",
    "Here, $ \\theta$ specifies the rotation around the $z$-axis and $\\phi$ defines the rotation around the $x$-axis. We can rotate the data in 3D, then project the data down onto the $(t_1,t_2)$ plane. Again, we are trying to capture as much variance in the data as we can, i.e., maximize the variance of $t_1$ and $t_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new data matrix which includes all three components of the original data, and scale\n",
    "X = df_moons[['x1', 'x2', 'x3']].values\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Define the rotation matrix, then rotate the scaled data\n",
    "theta = 0/4*np.pi\n",
    "phi = 0/4*np.pi\n",
    "Q = ( np.array([[np.cos(theta), -np.sin(theta), 0], [np.sin(theta), np.cos(theta), 0],[0, 0, 1]])\n",
    "     @np.array([[1, 0, 0], [0, np.cos(phi), -np.sin(phi)], [0, np.sin(phi), np.cos(phi)]]) )\n",
    "T = X @ Q\n",
    "\n",
    "# Create a 3D scatter plot of the rotated data, showing the projection onto the t1-t2 plane\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1.scatter(T[:,0], T[:,1], T[:,2], alpha = 0.5)\n",
    "ax1.scatter(T[:,0], T[:,1], 0*T[:,2] - 4, s = 2, c = 'k')\n",
    "ax1.axis('equal')\n",
    "\n",
    "# Use seaborn to show a joint plot of the projected data\n",
    "sns.jointplot(x=T[:,0], y=T[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to calculate the rotation matrix $ Q $ that will result in the maximum variance? **Yes**, this is exactly what Principal Component Analysis (PCA) does. We will use the `sklearn.decomposition.PCA` to create a PCA object, fit it to our standardized data, then extract the rotation matrix $ Q $ that will result in the maximimum variance in our projected data.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We create our standardized data matrix by applying the StandardScaler directly to the dataframe.\n",
    "# Note that StandardScaler returns a numpy array, not a dataframe\n",
    "X = StandardScaler().fit_transform(df_moons)\n",
    "\n",
    "# We create a PCA object and fit it to our standardized data\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# The rotation matrix \"Q\" is the transpose of the \"principal components\" learnt using PCA. \n",
    "# We can access it using the \"components_\" attribute of the PCA object after fitting to the data.\n",
    "Q = pca.components_.T   # Transpose to get the rotation matrix\n",
    "T = X @ Q\n",
    "\n",
    "# Create a 3D scatter plot of the rotated data, showing the projection onto the t1-t2 plane\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1.scatter(T[:,0], T[:,1], T[:,2], alpha = 0.5)\n",
    "ax1.scatter(T[:,0], T[:,1], 0*T[:,2] - 4, s = 2, c = 'k')\n",
    "ax1.axis('equal')\n",
    "\n",
    "sns.jointplot(x=T[:,0], y=T[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we used `pca.fit(X)` to learn the rotation matrix which will maximize the variance in our projected data. However, we usually don't need to extract the matrix $ Q $ separately, we can simply use `T = pca.transform(X)`, which automatically applies the learnt transformation. Note that you have to `fit` the PCA model before you can use `transform`.\n",
    "\n",
    "In literature, the columns of $ Q $ are often called the loadings, and the transformed data $ T $ are called the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df_moons)\n",
    "pca = PCA() # Create a PCA object\n",
    "pca.fit(X)  # Fit it to the data\n",
    "T = pca.transform(X)    # Transform the data to obtain the scores\n",
    "sns.jointplot(x=T[:,0], y=T[:,1])   #  Plot the first two components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fitting and transforming the data separately, we can use the `fit_transform` method to fit the data and calculate the scores directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df_moons)\n",
    "pca = PCA() # Create a PCA object\n",
    "T = pca.fit_transform(X)    # Transform the data to obtain the scores\n",
    "sns.jointplot(x=T[:,0], y=T[:,1])   #  Plot the first two components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced dimensions and variance explained by each principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the *shape* of the matrices $ Q $ and $ T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q.shape)\n",
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the $ Q $ matrix is a $ 3 × 3 $ matrix, whereas the scores matrix is $ 500 × 3 $. In other words, the scores matrix contains 500 observations and 3 feature - as many as the original data set. So did PCA perform dimensionality reduction?\n",
    "\n",
    "The effect of PCA is to produce a scores matrix $ T = XQ $ such that the first feature in our transformed data $t_1$ has the greatest variance, $t_2$ has the second greated variance, etc. By plotting the two-dimensional data $ (t_1, t_2) $, we are plotting the projection with the greatest variance.\n",
    "\n",
    "We can visualise the amount of variance in each principal component (i.e., the amount of variance in the scores $t_1$, $t_2$, etc.) using the `explained_variance_ratio_` method in the PCA object. The `explained_variance_ratio_` gives the *fraction* of variance explained by each component, i.e., $$ \\frac{\\sigma^2_{component}}{\\sigma^2_{total}} $$ \n",
    "\n",
    "Let us plot the fraction of variance explained by each component, as well as the cumulative variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=range(1,pca.n_components_+1)  # Used for the x-axis\n",
    "plt.bar(index, pca.explained_variance_ratio_, label='Individual components')\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulativeFractionVarianceExplained = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(index, cumulativeFractionVarianceExplained, 'o-', label='Cumulative variance explained')\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.hlines(0.9, 0, pca.n_components_, color='k', ls='--', label='90 percent of total variance')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance fraction')\n",
    "plt.grid(True)  # Add grid\n",
    "plt.legend()\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moons data, almost all the variance lies in the first principal component. This is not obvious from the seaborn jointplot, since the jointplot does not scale the axes to be equal. We can see that the 3rd principal component contains less than 1% of the variance.\n",
    "\n",
    "In practice, we can create a scores matrix $ T $ with reduced dimensions by only keeping the first few columns in $ Q $. Let's define $ Q_r $ as the first $ d $ columns in $ Q $, where $d$ is the number of dimensions we'd like to retain. In our case, we want to retain two dimensions (useful for visualisation). Then we simply have:\n",
    "\n",
    "$$ T = XQ_r $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df_moons)\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "Q = pca.components_.T[:,:2]   # We retain all the rows, but only the first two columns\n",
    "print(Q.shape)\n",
    "\n",
    "T = X @ Q\n",
    "print(T.shape)\n",
    "\n",
    "sns.jointplot(x=T[:,0], y=T[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we can set the number of components to retain by using the `n_components` keyword argument when creating the PCA object, then applying `fit_transform` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df_moons)\n",
    "pca = PCA(n_components=2) # Create a PCA object that will only retain the first two principal components\n",
    "T = pca.fit_transform(X)    # Transform the data to obtain the scores\n",
    "sns.jointplot(x=T[:,0], y=T[:,1])   #  Plot the first two components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to reconstruct the original data matrix using the scores. If we retained all components, then $Q$ will be a $3 × 3$ matrix, hence invertible:\n",
    "$$ T = XQ $$\n",
    "$$ \\therefore X = TQ^{-1} $$\n",
    "\n",
    "But orthogonal matrices (such as $Q$) have the special property that $Q^{-1} = Q^T$, i.e., the inverse is equal to the transpose:\n",
    "\n",
    "$$ X = TQ^T $$\n",
    "\n",
    "If we only have the first few principal components (i.e., we used $ T = XQ_r $ where $Q_r$ is only the first few rows in the rotation matrix), then we can estimate the original matrix in the same way:\n",
    "\n",
    "$$ X ≈ X_r = TQ_r^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df_moons)\n",
    "pca = PCA(n_components=2) # Create a PCA object that will only retain the first two principal components\n",
    "T = pca.fit_transform(X)    # Transform the data to obtain the scores\n",
    "Qr = pca.components_.T[:,:2] # Retain the first two rows of Q\n",
    "Xr = T @ Qr.T  # Reconstruct the original data\n",
    "MSE = ((X - Xr)**2).mean()  # Calculate the mean squared error\n",
    "\n",
    "print(f\"The mean squared error between the original and reconstructed data is {MSE:.2e}\")\n",
    "print(f\"The variance not explained by the first two principal components is {1 - pca.explained_variance_ratio_.sum():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error (MSE) between the original and reconstructed data is the same as the variance *not explained* by the retained principal components, which we know is a minimum per definition of PCA. The reconstruction error is a metric often used in fault detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA using the iron ore flotation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use PCA to visualise the iron ore flotation data. Start by loading the cleaned dataset with renamed columns. We will also resample to 4-hourly data to reduce the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concentrator = pd.read_csv(data_url['open_iof_cleaned'], index_col='date', parse_dates=['date'])\n",
    "df_concentrator = df_concentrator.resample('4h').median()\n",
    "\n",
    "df_concentrator.dropna(inplace=True)\n",
    "df_concentrator.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA and view the fraction variance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will look at a subset of the data that excludes the outputs of interest (i.e., iron and silica composition). We will try to see if the easily measurable data form \"clusters\" that we can visualise to identify regions of high and low compositions.\n",
    "\n",
    "We will perform PCA but retain all components so we can visualise the fraction of variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df_concentrator.drop(['y product iron %', 'y product silica %'], axis = 1)\n",
    "\n",
    "# Use StandardScaler to standardize the data, then use PCA to learn the scores \"T\"\n",
    "\n",
    "\n",
    "# Learn components: Explained variance plot\n",
    "index=range(1,pca.n_components_+1)\n",
    "plt.bar(index, pca.explained_variance_ratio_, label='Individual components')\n",
    "plt.plot(index, np.cumsum(pca.explained_variance_ratio_), 'o-', label='Cumulative variance explained')\n",
    "plt.hlines(0.9, 0, pca.n_components_, color='k', ls='--', label='90 percent of total variance')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance fraction')\n",
    "plt.grid(True)  # Add grid\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to retain at least ten components to explain 90% of the variance. We will keep this in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the first few principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us start with visualisation in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (5,5))\n",
    "\n",
    "# Create a scatter plot of the first two components, contained in the first two columns of the scores matrix \"T\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see some clusters in the data. We can provide more information by colouring the data according to values in our data frame. Let's colour the data according to the timestamp. We can do so by setting the colour of the data points in our `scatter` plot using the `c` keyword argument. We also set the colormap to `viridis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Create a scatter plot of the first two components, contained in the first two columns of the scores matrix \"T\"\n",
    "# Use the color argument to show the month of the year, by setting `c=df_concentrator.index.month`\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behaviour early on (months 4 and 5) in the dataset is quite different from the later months. Let's see how the other variables change with respect to the location in the reduced space. First, we define a function that will plot the first two principal components coloured according to time, then again coloured according to each feature in our data set. Here, we will use the full data set (including the product compositions) to colour the data.\n",
    "\n",
    "Since we will use this approach again later on, we define a function that accepts as inputs the scores to plot, as well as the dataframe containing the data used to colour the markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts scores (more generally, latent variables) \"L\" and dataframe \"df\" as inputs, with optional input \"alpha\"\n",
    "def plotAndColorLatentVariables(L, df, alpha = 0.5): \n",
    "    I = np.ceil(np.sqrt(df.shape[1])).astype(int)   # Determine the number of rows in the subplot grid\n",
    "    J = np.ceil(df.shape[1]/I).astype(int)          # Determine the number of columns in the subplot grid\n",
    "    fig, ax = plt.subplots(I,J, figsize = (20,20))  # Create the subplot grid\n",
    "    \n",
    "    for i in range(I):                       # Loop over the rows\n",
    "        for j in range(J):                   # Loop over the columns\n",
    "            k = I*i+j-1                      # Compute the index of the subplot\n",
    "            if k == -1:     # Colour the plot according to the dataframe index\n",
    "                ax[i,j].scatter(L[:, 0], L[:, 1], alpha = alpha, c=df.index, cmap='viridis')\n",
    "                ax[i,j].set_title('Index')\n",
    "                \n",
    "            elif k <= (df.shape[1]-1):  # Colour the plot according to the dataframe columns\n",
    "                ax[i,j].scatter(L[:, 0], L[:, 1], alpha = alpha, c=df.iloc[:,k], cmap='viridis')\n",
    "                ax[i,j].set_title(df.columns[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the function `plotAndColorLatentVariables` to plot the PCA scores and colour by the columns in our original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plotAndColorLatentVariables(\u001b[43mT\u001b[49m, df_concentrator)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "plotAndColorLatentVariables(T, df_concentrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualisation, we can see that the clusters are mostly related to the airflowrates. Many of the outliers are associated with low amine or starch flowrates. We can also see the forth depth having a major influence on the variability in the data, with froth depth typically increasing as $t_1$ and $t_2$ increase from the bottom left of the plot to the top right.\n",
    "\n",
    "Unfortunately, the relationship between the scores of the input data and the product composition is not clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biplot is another common way to interpret scores plot. The biplot projects the original coordinate system (i.e., the original variables) into the reduced dimensional space, which fortuitously corresponds to the columns of our loadings matrix $Q$. This gives an indication of the relative contribution of each variable to the principal component scores.\n",
    "\n",
    "See the projection of the 3D moons data, along with the projection of the coordinate axes, to understand the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('PCA-rotation-3D.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we select five variables (`x C01.air.flow Nm3/h`, `x C01.froth.depth mm`, `u starch m3/h`, `u amina m3/h`, `d feed m3/h`) to include in the biplot. We could plot them all, but that would make the plot far too confusing. \n",
    "\n",
    "To plot the loading vectors (i.e., the columns of $Q$ corresponding to the features of interest), we plot an arrow from the origin to the vector, and label it with the name of the corresponding feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (5,5))\n",
    "ax.scatter(T[:, 0], T[:, 1], c=df_concentrator.index.month, cmap='viridis') # Plot the scores, coloured by month\n",
    "Q = pca.components_.T   # Get the loadings\n",
    "\n",
    "selected_variables = ['x C01.air.flow Nm3/h', \n",
    "                      'x C01.froth.depth mm', \n",
    "                      'u starch m3/h',\n",
    "                      'u amina m3/h',\n",
    "                      'd feed m3/h']\n",
    "\n",
    "scaling_factor = 15     # The scaling factor adjusts the relative length of the arrows. Adjust as needed\n",
    "for variable in selected_variables:     # Loop over the selected variables\n",
    "        i = np.where(df_pca.columns == variable)[0]     # Find the index of the variable in the dataframe\n",
    "        ax.annotate(variable,                           # Create the arrow using the annotate  method\n",
    "                xy=(0,0),       # Arrow start position\n",
    "                xytext=(scaling_factor*Q[i,0], scaling_factor*Q[i,1]), # Arrow end position\n",
    "                arrowprops=dict(color='red',arrowstyle='<-',relpos=(0,0)) # Additional arrow properties\n",
    "                )\n",
    "\n",
    "ax.set_title('T1 vs T2 colored by plant.flotation.bank01.column03.air.flow')\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded to earlier, the airflow and froth depth have a major impact on the data, as shown by the large arrows. The biplot is a very useful way to summarize the contributions of multiple features in a single plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting in three dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing prevents you from plotting the first three principal components in the same way. This may also be a useful way to identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(T[:,0], T[:,1], T[:,2], c=df_concentrator.index.month, cmap='viridis')\n",
    "ax.set_xlabel('T1')\n",
    "ax.set_ylabel('T2')\n",
    "ax.set_zlabel('T3')\n",
    "ax.set_title('3D Scatter Plot of T1, T2, and T3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond linear dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have asked \"what is the matrix $Q$ such that the variance in $T = XQ$ is maximized in $t_1$, then $t_2$, etc. ?\". \n",
    "\n",
    "Stated differently, we could ask \"Find the reduced feature matrix $Q_r$ such that the reconstruction error $ ∑ (X - X_r)^2 $ is minimized, with the reconstructed data $X_r = TQ_r^T $ and $ T = XQ_r$\". \n",
    "\n",
    "However, there are two important points to note.\n",
    "\n",
    "First, we could ask a different question, e.g., \"what is the reduced feature matrix $ T = XQ_r $ that serves as the best linear predictor for an output $y$?\" \n",
    "\n",
    "In other words, \"Find the orthogonal matrix $Q_r$ which minimizes $∑ (y - y_{predicted})^2 $ where $y_{predicted} = T\\beta$ and $T = XQ_r$, with $\\beta$ a vector of linear regression coefficients.\"\n",
    "\n",
    "This is the question that *partial least squares* (PLS) seeks to answer. Similar techniques include *canonical correlation analysis* (CCA), and *independent component analysis* (ICA), and many more. \n",
    "\n",
    "Second, in all the above we were looking for a matrix $Q$ which optimizes some objective function. This constrains the possible transformations to *linear transformations* of the data. There are many other non-linear dimensionality reduction techniques available, although the hyperparameters for these methods are often difficult to tune. Common methods include:\n",
    "* Kernel methods (e.g., kPCA) which first transforms the data to a non-linear feature space.\n",
    "* Manifold learning (e.g., LLE, t-SNE), which aims to learn a lower dimensional, non-linear manifold containing the data\n",
    "* Neural networks (e.g., autoencoders), which include \"bottle neck\" layers with a small number of features in the network architecture\n",
    "\n",
    "We will consider one method, t-SNE, below. See the scikit-learn [decomposition tutorial](https://scikit-learn.org/stable/modules/decomposition.html#) or [manifold learning tutorial](https://scikit-learn.org/stable/modules/manifold.html) pages for more details regarding the methods available in the scikit-learn library. [Keras](https://keras.io/) is an excellent tool for building autoencoders; you can read more on the [Keras blog](https://blog.keras.io/building-autoencoders-in-keras.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using non-linear methods to visualise more than three principal component scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we needed to retain the first ten principal components to achieve 90% cumulative variance explained. However, it is difficult to visualise the first 10 principal components. We can use a manifold learning technique, *t-SNE* ([t-distributed Stochastic Neighbourhood Embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)) to reduce the 10 principal components to a two-dimensional space for visualisation.\n",
    "\n",
    "Why not simply perform t-SNE on the original data? This is an option, but the non-linear method requires more computational power, and can become prohibitively slow in a high dimensional feature space. The method may also suffer from the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Repeat PCA to avoid potential errors\n",
    "pca = PCA()\n",
    "tsne = TSNE()\n",
    "\n",
    "X = StandardScaler().fit_transform(df_pca)\n",
    "T = pca.fit_transform(X)\n",
    "L = tsne.fit_transform(T[:,:10])\n",
    "plotAndColorLatentVariables(L, df_concentrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there are more distinct clusters in the 10-dimensional principal component space which we are not able to clearly visualise when we simply project the data linearly to the first two principal components. However, t-SNE enables us to plot the 10D space in 2D, keeping points that are \"close\" together in the original space, close together in the two-dimensional space. For example, notice the *high air flow*, *low feed rate* cluster, as well as the *moderate air flow*, *low froth depth* cluster. \n",
    "\n",
    "Experiment by providing a different number of principal components to t-SNE using `L = tsne.fit_transform(T[:,:n])` where `n` is the number of components retained, observing both the change in the visualisation and the speed of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following:\n",
    "# 1. Import TSNE from sklearn.manifold and PCA from sklearn.decomposition\n",
    "# 2. Create a PCA and a TSNE object. Specify the number of components to be retained in PCA as a smaller number (e.g., 10)\n",
    "# 3. Standardize the data contained in \"df_pca\" using StandardScaler to create a data matrix \"X\"\n",
    "# 4. Fit the PCA object to the data matrix \"X\" and obtain the scores \"T\"\n",
    "# 5. Perform TSNE on the scores \"T\" to obtain the latent variables \"L\"\n",
    "# 6. Create a scatter plot of the latent variables \"L\" using the scatter method, and color the points according to the month of the year\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Manifold Approximation and Projection (UMAP)\n",
    "UMAP is a recently developed non-linear dimensionality reduction technique that bears some similarities to t-SNE. It is faster than t-SNE, and likely more suited to subsequent clustering. Full details can be found in the [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/).\n",
    "\n",
    "In the following, we will apply UMAP to the iron ore flotation data, using the first ten principal components as input. We will also use the `n_neighbors` and `min_dist` hyperparameters to control the size of the neighbourhood and the minimum distance between points in the reduced space. Varying these hyperparameters will change the dimensionality reduction results. UMAP also relies on a random seed, which means results may not always be exactly reproducible. You can set the random seed using the `random_state` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "umap = UMAP(n_neighbors=10, min_dist=0.1, random_state=42)\n",
    "L = umap.fit_transform(T[:,:10])\n",
    "plotAndColorLatentVariables(L, df_concentrator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
